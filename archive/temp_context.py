--- a/main_enhanced.py
+++ b/main_enhanced.py
@@
-from modules.telegram_utils import format_ai_text, chunk_text, strip_html_tags
+from modules.telegram_utils import format_ai_text, chunk_text, strip_html_tags
+from modules.context_manager import ContextManager, build_prompt_with_context
@@
 logger = logging.getLogger(__name__)
@@
+# Conversation context manager (Supabase-backed)
+ctx_mgr = ContextManager()
+
@@
 async def handle_text(update, context):
     chat_id = update.effective_chat.id
     text = (update.message.text or "").strip()
 
     if not text:
         formatted_message, parse_mode = format_ai_text("ë‚´ìš©ì´ ë¹„ì–´ ìˆì–´ìš”. í…ìŠ¤íŠ¸ë¥¼ ë³´ë‚´ì£¼ì„¸ìš”.")
         await context.bot.send_message(chat_id, formatted_message, parse_mode=parse_mode)
         return
 
-    # Use Gemini to handle all text inputs, letting it determine the intent
-    prompt = f"ì‚¬ìš©ìì˜ ìš”ì²­: {text}\n\nì´ ìš”ì²­ì— ëŒ€í•´ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ê±°ë‚˜, í•„ìš”í•œ ê²½ìš° ë¶„ì„/ìš”ì•½í•˜ì—¬ ì‘ë‹µí•´ì£¼ì„¸ìš”."
+    # Save user message
+    try:
+        user_id = update.effective_user.id
+    except Exception:
+        user_id = None
+    ctx_mgr.add(chat_id, user_id, "user", text, "text")
+
+    # Build prompt with recent context
+    prompt = build_prompt_with_context(ctx_mgr, chat_id, text)
     res = generate_text_safe(prompt)
     
     if res.get("ok"):
-        formatted_message, parse_mode = format_ai_text(res["text"])
+        answer = res["text"]
+        # Save assistant answer and maybe compress
+        ctx_mgr.add(chat_id, user_id, "assistant", answer, "text")
+        ctx_mgr.compress_if_needed(chat_id)
+        formatted_message, parse_mode = format_ai_text(answer)
         await context.bot.send_message(chat_id, formatted_message, parse_mode=parse_mode)
     else:
         formatted_message, parse_mode = format_ai_text("ìš”ì²­ ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. í‘œí˜„ì„ ì¡°ê¸ˆ ë°”ê¿” ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
         await context.bot.send_message(chat_id, formatted_message, parse_mode=parse_mode)
@@
 async def handle_voice_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
@@
-        # Transcribe
+        # Transcribe
         transcription = transcribe_audio(wav_path)
         if transcription == "ìŒì„± ì „ì‚¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.":
             formatted_message, parse_mode = format_ai_text(transcription)
             await update.message.reply_text(formatted_message, parse_mode=parse_mode)
             os.unlink(ogg_path)
             os.unlink(wav_path)
             return
 
-        # Analyze with Gemini
-        res = generate_text_safe(f"ìŒì„± ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ìš”ì•½í•´ì£¼ì„¸ìš”. ì¶œë ¥ì€ ë§ˆí¬ë‹¤ìš´ ì—†ì´ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n\n{transcription}")
+        # Store user voice transcript
+        try:
+            user_id = update.effective_user.id
+        except Exception:
+            user_id = None
+        ctx_mgr.add(chat_id, user_id, "user", f"[ìŒì„± ì „ì‚¬]\n{transcription}", "voice")
+
+        # Analyze with Gemini with context
+        prompt = build_prompt_with_context(ctx_mgr, chat_id, f"ìŒì„± ë‚´ìš©ì„ ë¶„ì„í•˜ê³  ìš”ì•½:\n{transcription}")
+        res = generate_text_safe(prompt)
         summary = res.get("text") if res.get("ok") else "ìŒì„± ë¶„ì„ ë° ìš”ì•½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤." 
 
         message = (
             "ğŸ¤ ìŒì„± ë¶„ì„ ê²°ê³¼:\n\n"
             f"ì „ì‚¬:\n{transcription}\n\n"
             f"ìš”ì•½:\n{summary}"
         )
         await update.message.reply_text(message)
+
+        # Save assistant answer
+        ctx_mgr.add(chat_id, user_id, "assistant", summary, "voice")
+        ctx_mgr.compress_if_needed(chat_id)
@@
 async def handle_image(update: Update, context: ContextTypes.DEFAULT_TYPE):
@@
-        analysis = res.get("text") if res.get("ok") else (res.get("error") or "ì´ë¯¸ì§€ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
+        analysis = res.get("text") if res.get("ok") else (res.get("error") or "ì´ë¯¸ì§€ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
         # Send chunked if too long
         for chunk in chunk_text(analysis):
             await update.message.reply_text(chunk)
+
+        # Add to context as assistant message for future reference
+        try:
+            user_id = update.effective_user.id
+        except Exception:
+            user_id = None
+        ctx_mgr.add(chat_id, user_id, "assistant", f"[ì´ë¯¸ì§€ ë¶„ì„]\n{analysis}", "image")
+        ctx_mgr.compress_if_needed(chat_id)
